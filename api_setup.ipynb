{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwardhongwang/2025_Spring_ES26_ES294-/blob/main/api_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dzeHYa5GCxN7"
      },
      "outputs": [],
      "source": [
        "# MIT License\n",
        "#\n",
        "# @title Copyright (c) 2025 Mauricio Tec { display-mode: \"form\" }\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13i7KQ9t-CV8"
      },
      "source": [
        "\n",
        "# Welcome to the Harvard ES 26 and ES 294 Recitation 3\n",
        "\n",
        "\n",
        "\n",
        "# **API Setup**\n",
        "\n",
        "\n",
        "\n",
        "Expected completion time: 20 min\n",
        "\n",
        "\n",
        "## March 10, 2025  <br> Edward Hong Wang\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TL;DR** In this notebook you will find instructions to setup your OpenAI and HuggingFace API keys for the tutorial.\n",
        "\n",
        "<!--\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Agfj2lsK155vzmvG6vB4dScH7RqUWV9B\" alt=\"drawing\" width=\"400\"/> -->\n",
        "\n",
        "<!-- <img src=\"https://drive.google.com/uc?export=view&id=11o2zAv2_Cu8BL-FVdoRY8z5IEruO3ElZ\" alt=\"drawing\" width=\"400\"/> -->\n",
        "\n",
        "<!-- https://drive.google.com/file/d/11o2zAv2_Cu8BL-FVdoRY8z5IEruO3ElZ/view?usp=sharing -->\n"
      ],
      "metadata": {
        "id": "rMVhWY7bxfNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "üîë First, we need an OpenAI API Key since we will use ChatGPT for many demos. Don't worry üòâ , you can easily switch to other LLM provider if you prefer. But you'll need to work out yourself setting up their API keys.\n",
        "\n",
        "\n",
        "ü§ó You will also need a HuggingFace token for interfacing directly with the LLM weights and code (as opposed to using their call APIs).\n",
        "\n",
        "üïµÔ∏è‚Äç‚ôÇÔ∏è In addition to the HuggingFace token, you need to request access to the LLamaModels that we'll be using in the demo.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1agwSn9ZKa7y-QgQ4YrmpKLwv2c7IZY_w\" alt=\"drawing\" width=\"400\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPO_LtqGh2jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open AI API Key\n",
        "\n",
        "\n",
        "**Step 1:** Obtain the OpenAI API key from the OpenAI platform website.\n",
        "\n",
        "1. Visit the [OpenAI Platform Website](https://auth.openai.com/). Create an account.\n",
        "2. Log in, go to `Dashboard` on the Top Menu, then `API keys` on the left sidebar. Finally, select `+Create new secret Key` from the top right corner.\n",
        "3. Choose a name for your key (e.g., `GoogleColab`). Below is the screenshot of the screen you should be at.\n",
        "\n",
        "4. Clock on `Create secret key`, and it will take you to your API key. You must copy this key and store it for the next step.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=14Mu4cgk1Js8Az79gQrkmsaaLSC5LXgEJ\" alt=\"drawing\" width=\"800\"/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1oqGFLgYKG_VWHuJRF0Wt3VXOV6qg78on\" alt=\"drawing\" width=\"800\"/>\n",
        "\n",
        "**Step 2:** Add to the Google Colab Notebook Secrets\n",
        "\n",
        "1. On a Colab notebook (e.g., this one), open the fourth menu of the left sidebar, which has a key icon üîë.\n",
        "\n",
        "2. Click on `+ Add new secret`. On the column named name, input `OPENAI_API_KEY`. Then on the column named Value, paste the value of the API key you previously copied.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1CjR_VZh_aMuQbZeRLxjrxpDfCQEbu7iB\" alt=\"drawing\" width=\"400\"/>\n"
      ],
      "metadata": {
        "id": "lfJrR6UJmp8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test OpenAI API\n",
        "\n",
        "Let's now test with a simple LLM call."
      ],
      "metadata": {
        "id": "UEnnwVhTufFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first install the require packages\n",
        "%pip install -q openai"
      ],
      "metadata": {
        "id": "-02EzY0iqyhT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "fGl3fPg7uqBh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "client = openai.OpenAI()\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about Ai and Machine learning\"}],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga_EVfXuu4dq",
        "outputId": "9a5d69c4-42e5-4e65-e7d3-368a3c5e6774"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-B9D6V5xzg0SDJrdkKD1hhtS5OZo8k', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Why did the AI break up with its algorithm?\\n\\nBecause it found someone more compatible‚Äîwith better ‚Äúlearning rates‚Äù!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741535023, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=23, prompt_tokens=16, total_tokens=39, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face\n",
        "\n",
        "The previous LLM call happen entiredly on a remote server hosted by OpenAI. Let's see if we can download an entire LLM and use it to generate text.\n",
        "\n",
        "**Step 1**: Obtain the HuggingFace token.\n",
        "\n",
        "1. Go the [HuggingFace portal](https://huggingface.co/). Log in (create an account if needed).\n",
        "2. Click on your avatar on the right corner (small circle), which will open the settings right sidebar. Then go to `Access Tokens`.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1kb45vIZCK1EWVumMuYCMrDwsu7NMPUSz\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "3. Select `+Create new token` in the new menu. Then select any name (e.g., `ColabTest`). the permission level (write is ok), and finally, click on `Create token` again. Lastly, copy the new key (copy button), which will be added to Colab in the next step in the same way as the Open AI key.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Vfayo4auZ3Y6_UgNknMmylkBDstY4WKU\" alt=\"drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "**Step 2:** Add to the Google Colab Notebook Secrets\n",
        "\n",
        "1. On a Colab notebook (e.g., this one), open the fourth menu of the left sidebar, which has a key on the icon üîë.\n",
        "\n",
        "2. Click on `+ Add new secret`. On the column named name, input `HF_TOKEN`. Then on the column named Value, paste the value of the API key you previously copied.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1CjR_VZh_aMuQbZeRLxjrxpDfCQEbu7iB\" alt=\"drawing\" width=\"400\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OwiqJQeqvvmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test HF Token"
      ],
      "metadata": {
        "id": "fQGOMWFg2mQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q transformers[torch] # HF main module for LLMs"
      ],
      "metadata": {
        "id": "4ZF3tYI-2lmS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "model = \"tiiuae/falcon-rw-1b\"\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "pipeline(\n",
        "    \"This is a test: \",\n",
        "    max_length=20,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "OQLp35W22i6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 3.2 Models on Hugging Face\n",
        "\n",
        "This step is important since we want to be able to use the new Meta/LLama 3.2 1B models.\n",
        "\n",
        "üßµ In general, 1B models are the smallest possible modern LLMs. But we when we need to finetune the LLM weights directly for a downstream task, we need many computational resources. Therefore, 1B models are more convenient for Google Colab.\n",
        "\n",
        "**Step: Request Access**\n",
        "\n",
        "1. Navigate to the [Meta Llama 3.2 1B Model Card page](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) on HuggingFace.\n",
        "2. Expand the LLAMA 3.2 COMMUNITY LICENSE AGREEMENT, fill the form, and Submit. (Note: You might need to confirm your HuggingFace email if you didn't do it earlier).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1pD27fsgpNf1F1emVSlTyWLQjdiLlNW-Y\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "3. Done. Now you just need to way for approval, which usually takes a few minutes only.\n",
        "\n"
      ],
      "metadata": {
        "id": "ro383eU01EQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Deepseek R1 (Distill Qwen 1.5B)\n"
      ],
      "metadata": {
        "id": "zCAKrTJX8MTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "pipe(messages)\n"
      ],
      "metadata": {
        "id": "kmQm8vncv7-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c5994381-6d97-4846-e0ae-135fb70ff3bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'userdata' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2da9f06d133e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load model directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'userdata' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}